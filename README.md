# Hey ðŸ‘‹ What's up?

<pre>
hi, i am <a href='https://linktr.ee/prithivsakthi/'>prithiv</a>!

i am a graduate engineer [ug 2024], information technology, <a href='https://www.gcee.ac.in/'>gcee</a>
focused on working in llm training and enhancements, improving multimodal ai capabilities.
</pre>



## Multimodal Models

| **Camel-Doc-OCR-080125** | **Lumian2-VLR-7B-Thinking** |
|---------------------------|------------------------------|
| [![HuggingFace](https://img.shields.io/badge/ðŸ¤—-HuggingFace-yellow)](https://huggingface.co/prithivMLmods/Camel-Doc-OCR-080125) | [![HuggingFace](https://img.shields.io/badge/ðŸ¤—-HuggingFace-yellow)](https://huggingface.co/prithivMLmods/Lumian2-VLR-7B-Thinking) |
| [![Collection](https://img.shields.io/badge/Collection-VLMs-blue)](https://huggingface.co/collections/prithivMLmods/multimodal-vlms-aug25-68a56aac39fe8084f3c168bd) | [![Collection](https://img.shields.io/badge/Collection-VLMs-blue)](https://huggingface.co/collections/prithivMLmods/multimodal-vlms-aug25-68a56aac39fe8084f3c168bd) |
| Advanced Qwen2.5-VL fine-tuned specialist for document retrieval, content extraction, and analysis recognition, delivering superior document comprehension with Opendoc2-Analysis-Recognition training. | High-fidelity vision-language reasoning system with explicit grounded reasoning, producing structured reasoning traces aligned with visual coordinates for explainable multimodal understanding and step-by-step analysis. |

| **Qwen2.5-VL-7B-Abliterated-Caption-it** | **DeepCaption-VLA-7B** |
|--------------------------------------------|-------------------------|
| [![HuggingFace](https://img.shields.io/badge/ðŸ¤—-HuggingFace-yellow)](https://huggingface.co/prithivMLmods/Qwen2.5-VL-7B-Abliterated-Caption-it) | [![HuggingFace](https://img.shields.io/badge/ðŸ¤—-HuggingFace-yellow)](https://huggingface.co/prithivMLmods/DeepCaption-VLA-7B) |
| [![Collection](https://img.shields.io/badge/Collection-Abliterated-purple)](https://huggingface.co/collections/prithivMLmods/vl-abliterated-caption-68a0443b63182e97a15c47a3) | [![Collection](https://img.shields.io/badge/Collection-DeepCaption-green)](https://huggingface.co/collections/prithivMLmods/deepcaption-attr-68b041172ebcb867e45c556a) |
| Uncensored captioning specialist generating highly detailed descriptions across complex visual categories and sensitive content, optimized for comprehensive image analysis across varying aspect ratios. | Precision image captioning model focused on defining visual properties, object attributes, and scene details with exceptional descriptive accuracy across diverse image spectrums and dimensional variations. |

| **Megalodon-OCR-Sync-0713** | **DREX-062225-exp** |
|------------------------------|---------------------|
| [![HuggingFace](https://img.shields.io/badge/ðŸ¤—-HuggingFace-yellow)](https://huggingface.co/prithivMLmods/Megalodon-OCR-Sync-0713) | [![HuggingFace](https://img.shields.io/badge/ðŸ¤—-HuggingFace-yellow)](https://huggingface.co/prithivMLmods/DREX-062225-exp) |
| [![Collection](https://img.shields.io/badge/Collection-OCR-orange)](https://huggingface.co/collections/prithivMLmods/captioning-ocr-doctable-687382e1da822008bb5c06f2) | [![Collection](https://img.shields.io/badge/Collection-VLMs-cyan)](https://huggingface.co/collections/prithivMLmods/multimodal-vlms-until-july25-688312e6b840e1e156f13027) |
| Qwen2.5-VL-3B specialist trained on 200K image pairs including 70K Corvus-OCR-Caption-Mix samples, optimized for document OCR captioning, image reasoning, and visual analysis across variational dimensions. | Document Retrieval and Extraction eXpert built on docscopeOCR-7B architecture, specialized for superior document analysis and information extraction with Opendoc2-Analysis-Recognition training optimization. |

## Ranking Models: Text Gen
(Open LLM Leaderboard)

| **Galactic-Qwen-14B-Exp2** | **Sombrero-Opus-14B-Elite5** |
|----------------------------|-------------------------------|
| [![HuggingFace](https://img.shields.io/badge/ðŸ¤—-HuggingFace-yellow)](https://huggingface.co/prithivMLmods/Galactic-Qwen-14B-Exp2) | [![HuggingFace](https://img.shields.io/badge/ðŸ¤—-HuggingFace-yellow)](https://huggingface.co/prithivMLmods/Sombrero-Opus-14B-Elite5) |
| [![Leaderboard](https://img.shields.io/badge/Rank-59-green?logo=huggingface)](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?search=prithivMLmods%2FGalactic-Qwen-14B-Exp2) | [![Leaderboard](https://img.shields.io/badge/Rank-104-orange?logo=huggingface)](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?search=prithivMLmods%2FSombrero-Opus-14B-Elite5) |
| Advanced reasoning powerhouse optimized for general-purpose intelligence, excelling in contextual understanding, logical deduction, and complex multi-step problem-solving with superior performance metrics. | Elite conversational AI fine-tuned using advanced chain-of-thought reasoning methodologies and specialized datasets, delivering enhanced comprehension, structured responses, and intelligent dialogue capabilities. |

| **Viper-Coder-v1.1** | **Dinobot-Opus-14B-Exp** |
|-----------------------|---------------------------|
| [![HuggingFace](https://img.shields.io/badge/ðŸ¤—-HuggingFace-yellow)](https://huggingface.co/prithivMLmods/Viper-Coder-v1.1) | [![HuggingFace](https://img.shields.io/badge/ðŸ¤—-HuggingFace-yellow)](https://huggingface.co/prithivMLmods/Dinobot-Opus-14B-Exp) |
| [![Leaderboard](https://img.shields.io/badge/Rank-250-red?logo=huggingface)](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?search=prithivMLmods%2FViper-Coder-v1.1) | [![Leaderboard](https://img.shields.io/badge/Rank-132-yellow?logo=huggingface)](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?search=prithivMLmods%2FDinobot-Opus-14B-Exp) |
| Specialized coding AI fine-tuned on synthetic datasets leveraging cutting-edge coding logits and CoT methodologies, optimizing chain-of-thought reasoning and advanced logical problem-solving for programming tasks. | High-performance abliterated model based on Qwen 2.5 architecture, engineered for superior reasoning, detailed explanations, and conversational excellence with enhanced contextual understanding and multi-step analytical capabilities. |

For more, visit: [![HuggingFace](https://img.shields.io/badge/ðŸ¤—-HuggingFace-yellow)](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?search=prithivMLmods)

## Other Pages

| **Stranger Zone** | **Stranger Guard** |
|-------------------|-------------------|
| [![HuggingFace](https://img.shields.io/badge/ðŸ¤—-HuggingFace-yellow)](https://huggingface.co/strangerzonehf) | [![HuggingFace](https://img.shields.io/badge/ðŸ¤—-HuggingFace-yellow)](https://huggingface.co/strangerguardhf) |
| [![GitHub](https://img.shields.io/badge/GitHub-Repository-blue?logo=github)](https://github.com/Stranger-Zone) | [![GitHub](https://img.shields.io/badge/GitHub-Repository-blue?logo=github)](https://github.com/Stranger-Guard) |
| Building illustration adapters for diffusion models, The Stranger Zone specializes in intelligence development, focusing on fine-tuning models for computer vision ; text-to-image specialized adapters (LoRA). | Stranger Guard specializes in building strict content moderation models, with a core focus on advanced computer vision tasks. Our team develops precision-driven AI systems capable of detecting, classifying, and moderating visual content at scale. |



